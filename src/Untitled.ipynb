{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spn.algorithms.SPMN import SPMN\n",
    "from spn.algorithms.EM import EM_optimization\n",
    "from spn.structure.Base import Sum, Product, Max\n",
    "from spn.structure.leaves.spmnLeaves.SPMNLeaf import State, Utility\n",
    "from spn.structure.Base import assign_ids, rebuild_scopes_bottom_up, get_nodes_by_type, Context\n",
    "from spn.structure.StatisticalTypes import MetaType\n",
    "from spn.algorithms.splitting.RDC import get_split_cols_RDC_py\n",
    "from spn.algorithms.SPMNHelper import get_ds_context\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from spn.algorithms.Inference import  likelihood\n",
    "from spn.algorithms.MPE import mpe\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import queue, time\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "from spn.io.Graphics import plot_spn\n",
    "from spn.algorithms.MEU import meu\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "class S_RSPMN:\n",
    "    def __init__(self,\n",
    "                dataset = \"crossing_traffic\",\n",
    "                debug = False,\n",
    "                debug1 = True,\n",
    "                apply_em = False,\n",
    "                mi_threshold = 0.01,\n",
    "                deep_match = True,\n",
    "                horizon = 3,\n",
    "                problem_depth = 10,\n",
    "                samples = 100000,\n",
    "                num_vars = None,\n",
    "                plot_path = \"data\"\n",
    "            ):\n",
    "        self.dataset = dataset\n",
    "        self.debug = debug\n",
    "        self.debug1 = debug1\n",
    "        self.apply_em = apply_em\n",
    "        self.mi_threshold = mi_threshold\n",
    "        self.deep_match = deep_match\n",
    "        self.horizon = horizon\n",
    "        self.problem_depth = problem_depth\n",
    "        self.samples = samples\n",
    "        self.num_vars = num_vars\n",
    "        self.plot_path = plot_path\n",
    "\n",
    "        self.s1_node_to_SIDs = dict()\n",
    "        self.SID_to_branch = dict()\n",
    "        self.branch_to_SIDs = dict()\n",
    "        self.SID_to_s2 = dict()\n",
    "        self.s1_to_s2s = dict()\n",
    "\n",
    "        if dataset == \"skill_teaching_rl\":\n",
    "            self.meta_types = [MetaType.STATE]+[MetaType.DISCRETE]*(num_vars-3)+[MetaType.REAL]+[MetaType.DISCRETE]+[MetaType.UTILITY]\n",
    "        else:\n",
    "            self.meta_types = [MetaType.STATE]+[MetaType.DISCRETE]*(num_vars-1)+[MetaType.UTILITY]\n",
    "        self.scope = [i for i in range(len(self.meta_types))]\n",
    "        self.s2_count = 0\n",
    "        self.s2_scope_idx = len(self.scope)\n",
    "        self.spmn = None\n",
    "\n",
    "        if dataset == \"repeated_marbles\":\n",
    "            partialOrder = [['s1'],['draw'],['result','reward']]\n",
    "            decNode=['draw']\n",
    "            utilNode=['reward']\n",
    "            scopeVars=['s1','draw','result','reward']\n",
    "        elif dataset == \"tiger\":\n",
    "            partialOrder = [['s1'],['observation'],['action'],['reward']]\n",
    "            decNode=['action']\n",
    "            utilNode=['reward']\n",
    "            scopeVars=['s1','observation','action','reward']\n",
    "        elif dataset == \"frozen_lake\":\n",
    "            partialOrder = [['s1'],['action'],['observation','reward']]\n",
    "            decNode=['action']\n",
    "            utilNode=['reward']\n",
    "            scopeVars=['s1','action','observation','reward']\n",
    "        elif dataset == \"nchain\":\n",
    "            partialOrder = [['s1'],['action'],['observation'],['reward']]\n",
    "            decNode=['action']\n",
    "            utilNode=['reward']\n",
    "            scopeVars=['s1','action','observation','reward']\n",
    "        elif dataset == \"elevators\":\n",
    "            decNode=['decision']\n",
    "                #     'close-door',\n",
    "                #     'move-current-dir',\n",
    "                #     'open-door-going-up',\n",
    "                #     'open-door-going-down',\n",
    "                # ]\n",
    "            obs = [\n",
    "                    # 'elevator-at-floor-0',\n",
    "                    # 'elevator-at-floor-1',\n",
    "                    'elevator-floor',# 'elevator-at-floor-2',\n",
    "                    'person-in-elevator-going-down',\n",
    "                    'elevator-dir',\n",
    "                    #'person-waiting-1',\n",
    "                    'person-waiting',#person-waiting-2',\n",
    "                    #'person-waiting-3',\n",
    "                    'person-in-elevator-going-up',\n",
    "                ]\n",
    "            utilNode=['reward']\n",
    "            scopeVars=['s1']+decNode+obs+['reward']\n",
    "            partialOrder = [['s1']]+[[x] for x in decNode]+[obs+['reward']]\n",
    "        elif dataset == \"skill_teaching\":\n",
    "            decNode=['decision']\n",
    "                #     'giveHint-1',\n",
    "                #     'giveHint-2',\n",
    "                #     'askProb-1',\n",
    "                #     'askProb-2',\n",
    "                # ]\n",
    "            obs = [\n",
    "                    'hintedRightObs-1',\n",
    "                    'hintedRightObs-2',\n",
    "                    'answeredRightObs-1',\n",
    "                    'answeredRightObs-2',\n",
    "                    'updateTurnObs-1',\n",
    "                    'updateTurnObs-2',\n",
    "                    'hintDelayObs-1',\n",
    "                    'hintDelayObs-2',\n",
    "                ]\n",
    "            utilNode=['reward']\n",
    "            #scopeVars=['s1']+obs+decNode+['reward']\n",
    "            #partialOrder = [['s1'],obs]+[[x] for x in decNode]+[['reward']]\n",
    "            scopeVars=['s1']+decNode+obs+['reward']\n",
    "            partialOrder = [['s1']]+[[x] for x in decNode]+[obs+['reward']]\n",
    "        elif dataset == \"skill_teaching_rl\":\n",
    "            decNode=['action']\n",
    "                #     'giveHint-1',\n",
    "                #     'giveHint-2',\n",
    "                #     'askProb-1',\n",
    "                #     'askProb-2',\n",
    "                # ]\n",
    "            obs = [\n",
    "                    'action-1',\n",
    "                    'hintedRightObs-1',\n",
    "                    'hintedRightObs-2',\n",
    "                    'answeredRightObs-1',\n",
    "                    'answeredRightObs-2',\n",
    "                    'updateTurnObs-1',\n",
    "                    'updateTurnObs-2',\n",
    "                    'hintDelayObs-1',\n",
    "                    'hintDelayObs-2',\n",
    "                    'reward-1'\n",
    "                ]\n",
    "            utilNode=['reward']\n",
    "            #scopeVars=['s1']+obs+decNode+['reward']\n",
    "            #partialOrder = [['s1'],obs]+[[x] for x in decNode]+[['reward']]\n",
    "            scopeVars=['s1']+obs+decNode+utilNode\n",
    "            partialOrder = [['s1'],obs,decNode,utilNode]\n",
    "        elif dataset == \"crossing_traffic\":\n",
    "            decNode=['decision']\n",
    "                #     'move-east',\n",
    "                #     'move-north',\n",
    "                #     'move-south',\n",
    "                #     'move-west'\n",
    "                # ]\n",
    "            # obs = [\n",
    "            #         'arrival-max-xpos-1',\n",
    "            #         'arrival-max-xpos-2',\n",
    "            #         'arrival-max-xpos-3',\n",
    "            #         'robot-at[$x1, $y1]',\n",
    "            #         'robot-at[$x1, $y2]',\n",
    "            #         'robot-at[$x1, $y3]',\n",
    "            #         'robot-at[$x2, $y1]',\n",
    "            #         'robot-at[$x2, $y2]',\n",
    "            #         'robot-at[$x2, $y3]',\n",
    "            #         'robot-at[$x3, $y1]',\n",
    "            #         'robot-at[$x3, $y2]',\n",
    "            #         'robot-at[$x3, $y3]',\n",
    "            #     ]\n",
    "            utilNode=['reward']\n",
    "            #scopeVars=['s1']+decNode+obs+['reward']\n",
    "            scopeVars = ['s1', 'robot_position', 'decision',  'arrival', 'reward']\n",
    "            #partialOrder = [['s1']]+[[x] for x in decNode]+[obs]+[['reward']]\n",
    "            partialOrder = [['s1'],['robot_position']]+[[x] for x in decNode]+[['arrival','reward']]\n",
    "            scope = [i for i in range(len(scopeVars))]\n",
    "        elif dataset == \"crossing_traffic_rl\":\n",
    "            decNode=['action']\n",
    "            utilNode=['reward']\n",
    "            scopeVars = ['s1', 'action-1', 'robot_position-1', 'arrive-1', 'reward-1', 'action', 'reward']\n",
    "            partialOrder = [['s1'],['action-1', 'robot_position-1', 'arrive-1', 'reward-1'],['action'],['reward']]\n",
    "            scope = [i for i in range(len(scopeVars))]\n",
    "        self.decNode = decNode\n",
    "        #self.obs=obs\n",
    "        self.utilNode = utilNode\n",
    "        self.scopeVars = scopeVars\n",
    "        self.partialOrder = partialOrder\n",
    "        self.dec_indices = [i for i in range(len(scopeVars)) if scopeVars[i] in decNode]\n",
    "        self.util_indices = [i for i in range(len(scopeVars)) if scopeVars[i] in utilNode]\n",
    "        self.bug_flag = False\n",
    "\n",
    "    def get_horizon_train_data(self, data, horizon):\n",
    "        nans_h=np.empty(data.shape)\n",
    "        nans_h[:,:,:] = np.nan\n",
    "        data = np.concatenate((data,nans_h),axis=1)\n",
    "        train_data_h = np.concatenate([data[:,i:self.problem_depth+i] for i in range(horizon)],axis=2)\n",
    "        # add nans for s1\n",
    "        nans=np.empty((train_data_h.shape[0],train_data_h.shape[1],1))\n",
    "        nans[:] = np.nan\n",
    "        train_data_h = np.concatenate((nans,train_data_h),axis=2)\n",
    "        return train_data_h\n",
    "\n",
    "    def get_horizon_params(self,partialOrder, decNode, utilNode, scopeVars, meta_types, horizon):\n",
    "        partialOrder_h = [] + partialOrder\n",
    "        # for i in range(1,horizon):\n",
    "        #     partialOrder_h += [[var+\"_t+\"+str(i) for var in s] for s in partialOrder[1:]]\n",
    "        # decNode_h = decNode+[decNode[j]+\"_t+\"+str(i) for i in range (1,horizon) for j in range(len(decNode))]\n",
    "        # utilNode_h = utilNode+[utilNode[j]+\"_t+\"+str(i) for i in range (1,horizon) for j in range(len(utilNode))]\n",
    "        for i in range(1,horizon):\n",
    "            partialOrder_h += [[var+str(i) for var in s] for s in partialOrder[1:]]\n",
    "        decNode_h = decNode+[decNode[j]+str(i) for i in range (1,horizon) for j in range(len(decNode))]\n",
    "        utilNode_h = utilNode+[utilNode[j]+str(i) for i in range (1,horizon) for j in range(len(utilNode))]\n",
    "        scopeVars_h = [var for infoset in partialOrder_h for var in infoset]\n",
    "        meta_types_h = meta_types+meta_types[1:]*(horizon-1)\n",
    "        return partialOrder_h, decNode_h, utilNode_h, scopeVars_h, meta_types_h\n",
    "\n",
    "    def replace_nextState_with_s2(self, spmn):\n",
    "        s1 = spmn.children[0]\n",
    "        self.s1_to_s2s[s1] = list()\n",
    "        scope_t1 = {i for i in range(self.s2_scope_idx)}\n",
    "        q = queue.Queue()\n",
    "        q.put(spmn)\n",
    "        while not q.empty():\n",
    "            node = q.get()\n",
    "            if isinstance(node, Product):\n",
    "                terminal = False\n",
    "                to_remove = []\n",
    "                for child in node.children:\n",
    "                    # if the child has no variables from the first timestep\n",
    "                    if len(set(child.scope) & scope_t1) == 0:\n",
    "                        # then remove it to be replaced with an s2 node\n",
    "                        to_remove.append(child)\n",
    "                        terminal = True\n",
    "                    else:\n",
    "                        q.put(child)\n",
    "                if terminal:\n",
    "                    for child in to_remove:\n",
    "                        node.children.remove(child)\n",
    "                    new_s2 = State(\n",
    "                            [self.s2_count,self.s2_count+1],\n",
    "                            [1],\n",
    "                            [self.s2_count],\n",
    "                            scope=self.s2_scope_idx\n",
    "                        )\n",
    "                    self.SID_to_s2[self.s2_count] = deepcopy(new_s2)\n",
    "                    node.children.append(self.SID_to_s2[self.s2_count])\n",
    "                    self.s1_to_s2s[s1].append(self.SID_to_s2[self.s2_count])\n",
    "                    self.s2_count += 1\n",
    "            elif isinstance(node, Max) or isinstance(node, Sum):\n",
    "                for child in node.children:\n",
    "                    if len(set(child.scope) & scope_t1) == 0:\n",
    "                        # then remove it to be replaced with an s2 node\n",
    "                        node.children.remove(child)\n",
    "                        new_s2 = State(\n",
    "                                [self.s2_count,self.s2_count+1],\n",
    "                                [1],\n",
    "                                [self.s2_count],\n",
    "                                scope=self.s2_scope_idx\n",
    "                            )\n",
    "                        self.SID_to_s2[self.s2_count] = deepcopy(new_s2)\n",
    "                        node.children.append(self.SID_to_s2[self.s2_count])\n",
    "                        self.s1_to_s2s[s1].append(self.SID_to_s2[self.s2_count])\n",
    "                        self.s2_count += 1\n",
    "                    else:\n",
    "                        q.put(child)\n",
    "        return spmn\n",
    "\n",
    "    # TODO replace this by using a placeholder for s2 as last infoset in partial order,\n",
    "    #  --- then just replace that placeholder using method above\n",
    "    def assign_s2(self, spmn):\n",
    "        s1 = spmn.children[0]\n",
    "        q = queue.Queue()\n",
    "        q.put(spmn)\n",
    "        while not q.empty():\n",
    "            node = q.get()\n",
    "            if isinstance(node, Max) or isinstance(node, Sum):\n",
    "                for child in node.children:\n",
    "                    if isinstance(node, Max) or isinstance(node, Sum) or isinstance(node, Product):\n",
    "                        q.put(child)\n",
    "                    else:\n",
    "                        node.children.remove(child)\n",
    "                        new_s2 = State(\n",
    "                                [self.s2_count,self.s2_count+1],\n",
    "                                [1],\n",
    "                                [self.s2_count],\n",
    "                                scope=self.s2_scope_idx\n",
    "                            )\n",
    "                        self.SID_to_s2[self.s2_count] = new_s2\n",
    "                        node.children.append(Product(\n",
    "                                children=[\n",
    "                                    child,\n",
    "                                    new_s2\n",
    "                                ]\n",
    "                            ))\n",
    "                        self.s2_count += 1\n",
    "            elif isinstance(node, Product):\n",
    "                is_terminal = True\n",
    "                for child in node.children:\n",
    "                    if isinstance(child, Max) or isinstance(child, Sum):\n",
    "                        is_terminal = False\n",
    "                if is_terminal:\n",
    "                    new_s2 = State(\n",
    "                            [self.s2_count,self.s2_count+1],\n",
    "                            [1],\n",
    "                            [self.s2_count],\n",
    "                            scope=self.s2_scope_idx\n",
    "                        )\n",
    "                    self.SID_to_s2[s2_count] = new_s2\n",
    "                    self.s1_to_s2s[s1].append(new_s2)\n",
    "                    node.children.append(new_s2)\n",
    "                    self.s2_count += 1\n",
    "                else:\n",
    "                    for child in node.children:\n",
    "                        q.put(child)\n",
    "        return spmn\n",
    "\n",
    "    def update_s_nodes(self):\n",
    "        nodes = get_nodes_by_type(self.spmn.spmn_structure)\n",
    "        for node in nodes:\n",
    "            if type(node)==State:\n",
    "                bin_repr_points = list(range(self.s2_count))\n",
    "                breaks = list(range(self.s2_count+1))\n",
    "                densities = []\n",
    "                for i in range(self.s2_count):\n",
    "                    if i in node.bin_repr_points:\n",
    "                        densities.append(node.densities[node.bin_repr_points.index(i)])\n",
    "                    else:\n",
    "                        densities.append(0)\n",
    "                node.bin_repr_points = bin_repr_points\n",
    "                node.breaks = breaks\n",
    "                node.densities = densities\n",
    "\n",
    "    def set_new_s1_vals(self, train_data, last_step_with_SID_idx, can_get_next_SID):\n",
    "        nans=np.empty((train_data.shape[0],train_data.shape[1],1))\n",
    "        nans[:] = np.nan\n",
    "        # s1 at t is s2 at t-1\n",
    "        train_data_s2 = np.concatenate((train_data,nans),axis=2)\n",
    "        prev_step_data = train_data_s2[\n",
    "                np.arange(train_data.shape[0]),\n",
    "                last_step_with_SID_idx\n",
    "            ]\n",
    "        prev_SIDs = np.unique(prev_step_data[:,0]).astype(int)\n",
    "        relevant_branches = list()\n",
    "        for SID in prev_SIDs:\n",
    "            if SID in self.SID_to_branch:\n",
    "                branch = self.SID_to_branch[SID]\n",
    "                if not branch in relevant_branches:\n",
    "                    relevant_branches.append(branch)\n",
    "        for branch in relevant_branches:\n",
    "            branch_data_indices = np.arange(train_data.shape[0])[\n",
    "                    np.logical_and(\n",
    "                            can_get_next_SID,\n",
    "                            np.isin(prev_step_data[:,0], self.branch_to_SIDs[branch])\n",
    "                        )\n",
    "                ]\n",
    "            branch = assign_ids(branch)\n",
    "            x = train_data_s2[\n",
    "                    branch_data_indices,\n",
    "                    last_step_with_SID_idx[branch_data_indices]\n",
    "                ]\n",
    "            #print(f\"x.shape: {x.shape}\")\n",
    "            #print(f\"branch: {branch}\")\n",
    "            #print(self.branch_to_SIDs[branch])\n",
    "            new_SIDs = mpe(\n",
    "                    branch,\n",
    "                    train_data_s2[\n",
    "                            branch_data_indices,\n",
    "                            last_step_with_SID_idx[branch_data_indices]\n",
    "                        ]\n",
    "                )[:,self.s2_scope_idx]\n",
    "            if np.any(np.isnan(new_SIDs)):\n",
    "                print(\"\\nfound nan SID assignment\\n\\n\")\n",
    "                new_SIDs[np.isnan(new_SIDs)] = -1\n",
    "            train_data[\n",
    "                    branch_data_indices,\n",
    "                    last_step_with_SID_idx[branch_data_indices]+1,\n",
    "                    0\n",
    "                ] = new_SIDs\n",
    "        self.spmn.spmn_structure = assign_ids(self.spmn.spmn_structure)\n",
    "        #new_s1s = mpe(spmn_t_structure, prev_step_data)[:,len(scopeVars)]\n",
    "        #train_data[:,t,0] = new_s1s\n",
    "        return train_data\n",
    "\n",
    "\n",
    "\n",
    "    def matches_state_branch(self, branch, train_data, SID_indices,\n",
    "            last_step_with_SID_idx):\n",
    "        branch_SIDs = self.branch_to_SIDs[branch]\n",
    "        branch_SIDs_in_data = np.isin(train_data[:,:,0].astype(int),branch_SIDs)\n",
    "        branch_sequence_indices = np.any(branch_SIDs_in_data, axis=1)\n",
    "        branch_step_indices = np.argmax(branch_SIDs_in_data, axis=1)\n",
    "        split_cols = get_split_cols_RDC_py(threshold=self.mi_threshold)\n",
    "        for i in range(0,self.horizon):\n",
    "            # select only sequences with sufficient remaining depth\n",
    "            branch_sequence_indices_i = np.logical_and(\n",
    "                    branch_sequence_indices,\n",
    "                    (branch_step_indices+i)<self.problem_depth\n",
    "                )\n",
    "            SID_indices_i = np.logical_and(\n",
    "                    SID_indices,\n",
    "                    (last_step_with_SID_idx+i)<self.problem_depth\n",
    "                )\n",
    "            branch_data = train_data[\n",
    "                    branch_sequence_indices_i,\n",
    "                    branch_step_indices[branch_sequence_indices_i]\n",
    "                ]\n",
    "            newSID_data = train_data[\n",
    "                    SID_indices_i,\n",
    "                    last_step_with_SID_idx[SID_indices_i]\n",
    "                ]\n",
    "            for j in range(1,i+1):\n",
    "                branch_data_j = train_data[\n",
    "                        branch_sequence_indices_i,\n",
    "                        branch_step_indices[branch_sequence_indices_i]+j\n",
    "                    ][:,1:]\n",
    "                newSID_data_j = train_data[\n",
    "                        SID_indices_i,\n",
    "                        last_step_with_SID_idx[SID_indices_i]+j\n",
    "                    ][:,1:]\n",
    "                branch_data = np.concatenate((branch_data[:,0].reshape(-1,1), branch_data_j), axis=1)\n",
    "                newSID_data = np.concatenate((newSID_data[:,0].reshape(-1,1), newSID_data_j), axis=1)\n",
    "            corr_test_data = np.append(newSID_data,branch_data,axis=0)\n",
    "            if corr_test_data.shape[0] == 0: continue\n",
    "            metatypes = self.meta_types# + self.meta_types[1:]*i\n",
    "            ds_context = Context(meta_types=metatypes)\n",
    "            ds_context.add_domains(corr_test_data)\n",
    "            scope = self.scope#[j for j in range(len(self.scope) + len(self.scope[1:])*i)]\n",
    "            #print(\"scope:\\t\"+str(scope))\n",
    "            print(\"corr_test_data.shape:\\t\"+str(corr_test_data.shape))\n",
    "            rdc_slices = split_cols(corr_test_data, ds_context, scope)\n",
    "            for correlated_var_set_cluster, correlated_var_set_scope, weight in rdc_slices:\n",
    "                if (0 in correlated_var_set_scope) and (len(correlated_var_set_scope) > 1):\n",
    "                    return False\n",
    "            # TODO: check to see if SID (scope 0) is clustered with any other variables\n",
    "            #   if SID is only clustered with decision value then ignore\n",
    "        print(\"match found!\")\n",
    "        return True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ################################ learn #####################################\n",
    "    def learn_s_rspmn(self, data, plot = False):\n",
    "        print(\"self.mi_threshold:\\t\"+str(self.mi_threshold))\n",
    "        nans=np.empty((data.shape[0],data.shape[1],1))\n",
    "        nans[:] = np.nan\n",
    "        train_data = np.concatenate((nans,data),axis=2)\n",
    "        train_data[:,0,0]=0\n",
    "        print(\"got train_data\")\n",
    "        # merge sequence steps based on horizon\n",
    "        train_data_h = self.get_horizon_train_data(data, self.horizon)\n",
    "        print(\"got train_data_h\")\n",
    "        # s1 for step 1 is 0\n",
    "        train_data_h[:,0,0]=0\n",
    "\n",
    "        partialOrder_h, decNode_h, utilNode_h, scopeVars_h, meta_types_h = self.get_horizon_params(\n",
    "                self.partialOrder, self.decNode, self.utilNode, self.scopeVars, self.meta_types, self.horizon\n",
    "            )\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        spmn0 = SPMN(\n",
    "                partialOrder_h,\n",
    "                decNode_h,\n",
    "                utilNode_h,\n",
    "                scopeVars_h,\n",
    "                meta_types_h,\n",
    "                cluster_by_curr_information_set=True,\n",
    "                util_to_bin = False\n",
    "            )\n",
    "        if True:\n",
    "            print(\"start learning spmn0\")\n",
    "            spmn0_structure = spmn0.learn_spmn(train_data_h[:,0])\n",
    "            spmn0_stoptime = time.perf_counter()\n",
    "            spmn0_runtime = spmn0_stoptime - start_time\n",
    "            print(\"learining spmn0 runtime:\\t\" + str(spmn0_runtime))\n",
    "            print(\"spmn0 nodes:\\t\" + str(len(get_nodes_by_type(spmn0_structure))))\n",
    "            file = open(f\"spmn_0.pkle\",'wb')\n",
    "            import pickle\n",
    "            pickle.dump(spmn0_structure, file)\n",
    "            file.close()\n",
    "        else:\n",
    "            file = open(f\"spmn_0.pkle\",'rb')\n",
    "            import pickle\n",
    "            spmn0_structure = pickle.load(file)\n",
    "            file.close()\n",
    "\n",
    "        print(\"spmn0 meu:\\t\"+str(meu(spmn0_structure, np.array([[np.nan]*len(scopeVars_h)]))))\n",
    "\n",
    "        if plot:\n",
    "            from spn.io.Graphics import plot_spn\n",
    "            print(\"plotting spmn0\")\n",
    "            plot_spn(spmn0_structure, f\"{self.plot_path}/spmn0.png\", feature_labels=scopeVars_h)\n",
    "\n",
    "        self.s2_count = 1\n",
    "        spmn0_structure = self.replace_nextState_with_s2(spmn0_structure) # s2 is last scope index\n",
    "        spmn0_structure = assign_ids(spmn0_structure)\n",
    "        spmn0_structure = rebuild_scopes_bottom_up(spmn0_structure)\n",
    "        # update state nodes to contain probabilities for all state values\n",
    "        self.SID_to_branch[0] = spmn0_structure\n",
    "        self.branch_to_SIDs[spmn0_structure] = [0]\n",
    "        self.s1_node_to_SIDs[spmn0_structure.children[0]] = [0]\n",
    "\n",
    "        if plot:\n",
    "            from spn.io.Graphics import plot_spn\n",
    "            print(\"plotting spmn0 with s2 nodes\")\n",
    "            plot_spn(spmn0_structure,f\"{self.plot_path}/spmn0_with_s2.png\", feature_labels=self.scopeVars+[\"s2\"])\n",
    "\n",
    "        spmn_t = SPMN(\n",
    "                self.partialOrder,\n",
    "                self.decNode,\n",
    "                self.utilNode,\n",
    "                self.scopeVars,\n",
    "                self.meta_types,\n",
    "                cluster_by_curr_information_set=True,\n",
    "                util_to_bin = False\n",
    "            )\n",
    "        spmn_t_structure = Sum(weights=[1],children=[spmn0_structure])\n",
    "        spmn_t_structure = assign_ids(spmn_t_structure)\n",
    "        spmn_t_structure = rebuild_scopes_bottom_up(spmn_t_structure)\n",
    "        spmn_t.spmn_structure = spmn_t_structure\n",
    "        self.spmn = spmn_t\n",
    "        self.update_s_nodes()\n",
    "\n",
    "        done = False\n",
    "        total_pushing_SIDs_time = 0\n",
    "        total_time_learning_structures = 0\n",
    "        total_time_matching = 0\n",
    "        while True:\n",
    "            current_total_runtime = time.perf_counter() - start_time\n",
    "            print(\"\\n\\nruntime so far:\\t\" + str(current_total_runtime)+\"\\tnum_branches:\\t\"+str(len(self.spmn.spmn_structure.children)))\n",
    "            percent_time_pushing_SIDs = (total_pushing_SIDs_time / current_total_runtime)*100\n",
    "            print(\"percent_time_pushing_SIDs:\\t%.2f\" % percent_time_pushing_SIDs)\n",
    "            percent_time_learning_structures = (total_time_learning_structures / current_total_runtime)*100\n",
    "            print(\"percent_time_learning_structures:\\t%.2f\" % percent_time_learning_structures)\n",
    "            percent_time_matching = (total_time_matching / current_total_runtime)*100\n",
    "            print(\"percent_time_matching:\\t%.2f\" % percent_time_matching)\n",
    "            # push sequences forward through the existing structure until they all\n",
    "            #   reach an SID which has not yet been linked to a branch.\n",
    "            start_pushing_SIDs_time = time.perf_counter()\n",
    "            while True:\n",
    "                last_step_with_SID_idx = (np.argmax(np.isnan(train_data[:,:,0]), axis=1)-1).astype(int)\n",
    "                last_step_with_SID_idx[last_step_with_SID_idx==-1] = self.problem_depth-1\n",
    "                remaining_steps = np.sum(np.isnan(train_data[:,:,0]),axis=1)\n",
    "                last_step_already_modeled = np.isin(\n",
    "                        train_data[\n",
    "                                np.arange(train_data.shape[0]),\n",
    "                                last_step_with_SID_idx\n",
    "                            ][:,0],\n",
    "                        list(self.SID_to_branch.keys())\n",
    "                    )\n",
    "                can_get_next_SID = np.logical_and(last_step_already_modeled, remaining_steps > 0)\n",
    "                # if any sequences' last processed step have SIDs which match to\n",
    "                #   existing branches and have steps remaining:\n",
    "                if np.any(can_get_next_SID) and np.any(np.isnan(train_data[:,:,0])):\n",
    "                    # get the next SID\n",
    "                    train_data = self.set_new_s1_vals(train_data, last_step_with_SID_idx, can_get_next_SID)\n",
    "                    train_data_h[:,:,0] = train_data[:,:,0]\n",
    "                else:\n",
    "                    break\n",
    "            pushing_SIDs_time = time.perf_counter() - start_pushing_SIDs_time\n",
    "            total_pushing_SIDs_time += pushing_SIDs_time\n",
    "            # once all sequences have reached a stopping point, find the unlinked\n",
    "            #   SID with the most data waiting behind it.\n",
    "            max_data_val = 0\n",
    "            max_val_SID = None\n",
    "            max_val_SID_indices = None\n",
    "            unmatched_SID = False\n",
    "            for SID in range(1, self.s2_count):\n",
    "                if not SID in self.SID_to_branch:\n",
    "                    unmatched_SID = True\n",
    "                    SID_indices = train_data[\n",
    "                                np.arange(train_data.shape[0]),\n",
    "                                last_step_with_SID_idx\n",
    "                            ][:,0]==SID\n",
    "                        #np.arange(train_data.shape[0])[\n",
    "                        #     train_data[\n",
    "                        #             np.arange(train_data.shape[0]),\n",
    "                        #             last_step_with_SID_idx\n",
    "                        #         ][:self.s2_scope_idx]==SID\n",
    "                        # ]\n",
    "                    SID_data_val = np.sum(remaining_steps[SID_indices]+1)\n",
    "                    # print(\"SID_data_val:\\t\"+str(SID_data_val))\n",
    "                    # if SID_data_val == 0:\n",
    "                    #     print(\"0 val SID:\\t\"+str(SID))\n",
    "                    #     print(\"self.SID_to_branch:\\n\"+str(self.SID_to_branch))\n",
    "                    if SID_data_val > max_data_val:\n",
    "                        max_data_val = SID_data_val\n",
    "                        max_val_SID = SID\n",
    "                        max_val_SID_indices = SID_indices\n",
    "            if not np.any(last_step_with_SID_idx < self.problem_depth) or not unmatched_SID or max_data_val==0:\n",
    "                break\n",
    "            # look for an existing branch that adequately models the data corresponding\n",
    "            #   to this SID\n",
    "            matched = False\n",
    "            print(f\"\\nstart matching for SID {max_val_SID}\")\n",
    "            print(\"max_data_val:\\t\"+str(max_data_val)+\"\\tremaining_data:\\t\"+str(np.sum(remaining_steps)))\n",
    "            print(\"max_val_SID_indices:\\t\"+str(max_val_SID_indices))\n",
    "            start_matching_time = time.perf_counter()\n",
    "            for branch in self.spmn.spmn_structure.children:\n",
    "                if self.matches_state_branch(branch, train_data, max_val_SID_indices,\n",
    "                        last_step_with_SID_idx):\n",
    "                    self.branch_to_SIDs[branch].append(max_val_SID)\n",
    "                    branch_SIDs = self.branch_to_SIDs[branch]\n",
    "                    branch_s1_node = branch.children[0]\n",
    "                    densities = branch_s1_node.densities\n",
    "                    for SID in branch_SIDs:\n",
    "                        densities[SID] = 1\n",
    "                    branch_s1_node.densities = densities\n",
    "                    # link s2 for new_val to this s1 node\n",
    "                    self.SID_to_s2[max_val_SID].interface_links[branch_s1_node] = np.sum(max_val_SID_indices)\n",
    "                    self.s1_node_to_SIDs[branch_s1_node] = branch_SIDs\n",
    "                    self.SID_to_branch[max_val_SID] = branch\n",
    "                    weights = []\n",
    "                    for child in self.spmn.spmn_structure.children:\n",
    "                        child_s1_vals = self.branch_to_SIDs[child]\n",
    "                        count_child = 0 #np.sum(np.isin(train_data_unrolled[:,0],child_s1_vals))\n",
    "                        for s1_val in child_s1_vals:\n",
    "                            if s1_val == 0:\n",
    "                                count_child += train_data.shape[0] # 1 starting state for each sequence\n",
    "                            else:\n",
    "                                count_child += self.SID_to_s2[s1_val].interface_links[child.children[0]]\n",
    "                        prob_child = count_child / (self.samples * self.problem_depth)\n",
    "                        weights.append(prob_child)\n",
    "                    normalized_weights = np.array(weights) / np.sum(weights)\n",
    "                    self.spmn.spmn_structure.weights = normalized_weights.tolist()\n",
    "                    matched = True\n",
    "                    # as each branch is created to model a different distribution,\n",
    "                    #   we can expect that no further matches will be found.\n",
    "                    break\n",
    "            matching_time = time.perf_counter() - start_matching_time\n",
    "            total_time_matching += matching_time\n",
    "            start_time_learning_structure = time.perf_counter()\n",
    "            if not matched:\n",
    "                ################ < creating new branch for state   #############\n",
    "                h = self.horizon\n",
    "                tdh = self.get_horizon_train_data(data, h)\n",
    "                tdh[:,:,0] = train_data[:,:,0]\n",
    "                while True:\n",
    "                    new_spmn_data = tdh[max_val_SID_indices, last_step_with_SID_idx[max_val_SID_indices]]\n",
    "                    new_spmn_sl_data = new_spmn_data[~np.any(np.isnan(new_spmn_data),axis=1)]\n",
    "                    if new_spmn_sl_data.shape[0] > 100:\n",
    "                        break\n",
    "                    elif h <= 2:\n",
    "                        print(f\"\\n\\th=1 for {max_val_SID}\\n\")\n",
    "                        h = 1\n",
    "                        new_spmn_sl_data = train_data[max_val_SID_indices, last_step_with_SID_idx[max_val_SID_indices]]\n",
    "                        print(\"new_spmn_sl_data.shape:\\t\"+str(new_spmn_sl_data.shape))\n",
    "                        new_spmn_sl_data = np.concatenate((new_spmn_sl_data,np.ones((new_spmn_sl_data.shape[0],1))), axis=1)\n",
    "                        print(\"new_spmn_sl_data.shape:\\t\"+str(new_spmn_sl_data.shape))\n",
    "                        break\n",
    "                    else:\n",
    "                        h -= 1\n",
    "                        tdh = self.get_horizon_train_data(data, h)\n",
    "                        tdh[:,:,0] = train_data[:,:,0]\n",
    "                new_spmn_em_data = train_data[max_val_SID_indices, last_step_with_SID_idx[max_val_SID_indices]]\n",
    "                em_nans = np.empty((new_spmn_em_data.shape[0],1))\n",
    "                new_spmn_em_data = np.concatenate((new_spmn_em_data,em_nans),axis=1)\n",
    "                partialOrder_h, decNode_h, utilNode_h, scopeVars_h, meta_types_h = self.get_horizon_params(\n",
    "                        self.partialOrder, self.decNode, self.utilNode, self.scopeVars, self.meta_types, h\n",
    "                    )\n",
    "                if h == 1:\n",
    "                    partialOrder_h.append([\"dummy\"])\n",
    "                    scopeVars_h.append(\"dummy\")\n",
    "                    meta_types_h.append(MetaType.DISCRETE)\n",
    "                spmn_new_s1 = SPMN(\n",
    "                        partialOrder_h,\n",
    "                        decNode_h,\n",
    "                        utilNode_h,\n",
    "                        scopeVars_h,\n",
    "                        meta_types_h,\n",
    "                        cluster_by_curr_information_set=True,\n",
    "                        util_to_bin = False\n",
    "                    )\n",
    "                branch_num = len(self.spmn.spmn_structure.children)\n",
    "                percentage_of_data_sl = (new_spmn_sl_data.shape[0]/self.samples)*100\n",
    "                percentage_of_data_em = (new_spmn_em_data.shape[0]/self.samples)*100\n",
    "                print(f\"\\ncreating branch {branch_num} for SID {max_val_SID}, \\npercentage of data for SL: {percentage_of_data_sl}%, \\npercentage of data for EM: {percentage_of_data_em}%\")\n",
    "                remaining_data = np.sum(remaining_steps)\n",
    "                print(f\"total remaining data: {remaining_data}\")\n",
    "                # print(\"\\nnew_spmn_data[:10]:\\n\"+str(new_spmn_data[:10]))\n",
    "                # print(\"\\nlast_step_with_SID_idx[:5]:\\n\"+str(last_step_with_SID_idx[:5]))\n",
    "                # print(\"\\ntrain_data[:5]:\\n\"+str(train_data[:5]))\n",
    "                spmn_new_s1_structure = spmn_new_s1.learn_spmn(new_spmn_sl_data)\n",
    "                if h > 1:\n",
    "                    spmn_new_s1_structure = self.replace_nextState_with_s2(spmn_new_s1_structure)\n",
    "                else:\n",
    "                    print(f\"\\n\\th = 1 for SID {max_val_SID}\")\n",
    "                    spmn_new_s1_structure = self.replace_nextState_with_s2(spmn_new_s1_structure)\n",
    "                    # from spn.io.Graphics import plot_spn\n",
    "                    # spmn_new_s1_structure = assign_ids(spmn_new_s1_structure)\n",
    "                    # plot_spn(spmn_new_s1_structure, \"replaced_dummies.png\", feature_labels=self.scopeVars+[\"s2\"])\n",
    "                    # spmn_new_s1_structure = self.assign_s2(spmn_new_s1_structure)\n",
    "                # print(\"perfoming EM optimization\")\n",
    "                # EM_optimization(spmn_new_s1_structure, new_spmn_em_data, iterations=1, skip_validation=True)\n",
    "                branch_s1_node = spmn_new_s1_structure.children[0]\n",
    "                self.SID_to_branch[max_val_SID] = spmn_new_s1_structure\n",
    "                self.SID_to_s2[max_val_SID].interface_links[branch_s1_node] = np.sum(max_val_SID_indices)\n",
    "                self.s1_node_to_SIDs[branch_s1_node] = [max_val_SID]\n",
    "                self.branch_to_SIDs[spmn_new_s1_structure] = [max_val_SID]\n",
    "                self.spmn.spmn_structure.children += [spmn_new_s1_structure]\n",
    "                weights = []\n",
    "                for child in self.spmn.spmn_structure.children:\n",
    "                    child_s1_vals = self.branch_to_SIDs[child]\n",
    "                    count_child = 0 #np.sum(np.isin(train_data_unrolled[:,0],child_s1_vals))\n",
    "                    for s1_val in child_s1_vals:\n",
    "                        if s1_val == 0:\n",
    "                            count_child += train_data.shape[0] # 1 starting state for each sequence\n",
    "                        else:\n",
    "                            count_child += self.SID_to_s2[s1_val].interface_links[child.children[0]]\n",
    "                    prob_child = count_child / (self.samples * self.problem_depth)\n",
    "                    weights.append(prob_child)\n",
    "                normalized_weights = np.array(weights) / np.sum(weights)\n",
    "                self.spmn.spmn_structure.weights = normalized_weights.tolist()\n",
    "                self.update_s_nodes()\n",
    "                self.spmn.spmn_structure = assign_ids(self.spmn.spmn_structure)\n",
    "                self.spmn.spmn_structure = rebuild_scopes_bottom_up(self.spmn.spmn_structure)\n",
    "            time_learning_structure = time.perf_counter() - start_time_learning_structure\n",
    "            total_time_learning_structures += time_learning_structure\n",
    "        learn_s_rspmn_stoptime = time.perf_counter()\n",
    "        learn_s_rspmn_runtime = learn_s_rspmn_stoptime - start_time\n",
    "        print(f\"\\n\\nlearn_s_rspmn runtime: {learn_s_rspmn_runtime}\\n\\n\")\n",
    "        self.learning_time = learn_s_rspmn_runtime\n",
    "        num_nodes = len(get_nodes_by_type(self.spmn.spmn_structure))\n",
    "        print(f\"num nodes:\\t {num_nodes}\")\n",
    "        if plot:\n",
    "            from spn.io.Graphics import plot_spn\n",
    "            plot_spn(self.spmn.spmn_structure, f\"{self.plot_path}/s-rspmn.png\", feature_labels=self.scopeVars+[\"s2\"])\n",
    "            plot_spn(self.spmn.spmn_structure, f\"{self.plot_path}/s-rspmn_interfaces.png\", feature_labels=self.scopeVars+[\"s2\"], draw_interfaces=True)\n",
    "        return train_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_branch_to_decisions_to_s2(rspmn_root):\n",
    "    branch_and_decisions_to_s2 = dict()\n",
    "    for branch in rspmn_root.children:\n",
    "        queue = branch.children[1:]\n",
    "        fill_branch_and_decisions_to_s2(branch_and_decisions_to_s2, queue, [branch])\n",
    "    branch_to_decisions_to_s2s = dict()\n",
    "    for branch_and_decisions, s2 in branch_and_decisions_to_s2.items():\n",
    "        branch = branch_and_decisions[0]\n",
    "        decision_path = branch_and_decisions[1:]\n",
    "        if branch in branch_to_decisions_to_s2s:\n",
    "            branch_to_decisions_to_s2s[branch][decision_path] = s2\n",
    "        else:\n",
    "            branch_to_decisions_to_s2s[branch] = {decision_path: s2}\n",
    "    return branch_to_decisions_to_s2s\n",
    "\n",
    "def fill_branch_and_decisions_to_s2(branch_and_decisions_to_s2, queue, path):\n",
    "    while len(queue) > 0:\n",
    "        node = queue.pop(0)\n",
    "        if isinstance(node, Max):\n",
    "            for i in range(len(node.dec_values)):\n",
    "                dec_val_i = node.dec_values[i]\n",
    "                child_i = node.children[i]\n",
    "                fill_branch_and_decisions_to_s2(\n",
    "                        branch_and_decisions_to_s2,\n",
    "                        [child_i],\n",
    "                        path+[dec_val_i]\n",
    "                    )\n",
    "        elif isinstance(node, State):\n",
    "            if tuple(path) in branch_and_decisions_to_s2:\n",
    "                branch_and_decisions_to_s2[tuple(path)] += [node]\n",
    "            else:\n",
    "                branch_and_decisions_to_s2[tuple(path)] = [node]\n",
    "        elif isinstance(node, Product) or isinstance(node, Sum):\n",
    "            for child in node.children:\n",
    "                queue.append(child)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def rmeu(rspmn, input_data, depth, debug=False):\n",
    "    assert not np.isnan(input_data[0]), \"starting SID (input_data[0]) must be defined.\"\n",
    "    root = rspmn.spmn.spmn_structure\n",
    "    if not input_data[0] in rspmn.SID_to_branch:\n",
    "        print(\"SID_to_branch cache miss\")\n",
    "        return 0\n",
    "    branch = rspmn.SID_to_branch[input_data[0]]\n",
    "    branch_s2s = rspmn.s1_to_s2s[branch.children[0]]\n",
    "    work_branch = deepcopy(branch)\n",
    "    if depth > 1 and len(branch_s2s[0].interface_links) == 0:\n",
    "        print(\"\\nOOF\\n\")\n",
    "        return None # unlinked branches cannot be evaluated beyond depth 1\n",
    "    work_branch = assign_ids(work_branch)\n",
    "    # set up caches\n",
    "    if not hasattr(rspmn,\"branch_and_depth_to_rmeu\"):\n",
    "        branch_and_depth_to_rmeu = dict()\n",
    "        setattr(rspmn,\"branch_and_depth_to_rmeu\",branch_and_depth_to_rmeu)\n",
    "    max_EU = None\n",
    "    # if unconditioned meu for this state branch and depth has already been cached, just return the cached value\n",
    "    if np.all(np.isnan(input_data[1:])):\n",
    "        if (branch, depth) in rspmn.branch_and_depth_to_rmeu:\n",
    "            root = assign_ids(root)\n",
    "            return rspmn.branch_and_depth_to_rmeu[(branch, depth)]\n",
    "        elif depth == 1:\n",
    "            max_EU = meu(work_branch, np.array([input_data])).reshape(-1)\n",
    "            rspmn.branch_and_depth_to_rmeu[(branch, depth)] = max_EU\n",
    "            root = assign_ids(root)\n",
    "            return max_EU\n",
    "    elif depth == 1:\n",
    "        max_EU = meu(work_branch, np.array([input_data])).reshape(-1)\n",
    "        root = assign_ids(root)\n",
    "        return max_EU\n",
    "    SID_to_util = dict()\n",
    "    for s2 in branch_s2s:\n",
    "        SID = np.argmax(s2.densities).astype(int)\n",
    "        next_data = np.array([SID]+[np.nan]*(rspmn.num_vars+1))\n",
    "        s2_value = rmeu(rspmn, next_data, depth-1)\n",
    "        # b = rspmn.SID_to_branch[SID]\n",
    "        # bnum = rspmn.spmn.spmn_structure.children.index(b)\n",
    "        # print(f\"Branch: {bnum},  depth: {depth-1},  s2_value: {s2_value}\")\n",
    "        if s2_value is None: SID_to_util[SID] = None\n",
    "        else:\n",
    "            SID_to_util[SID] = Utility(\n",
    "                    [s2_value,s2_value+1],\n",
    "                    [1],\n",
    "                    [s2_value],\n",
    "                    scope=rspmn.s2_scope_idx\n",
    "                )\n",
    "    q = work_branch.children[1:]\n",
    "    while len(q) > 0:\n",
    "        node = q.pop(0)\n",
    "        if isinstance(node, Max) or isinstance(node, Sum) or isinstance(node, Product):\n",
    "            for i in range(len(node.children)):\n",
    "                if isinstance(node.children[i], State):\n",
    "                    SID = np.argmax(node.children[i].densities).astype(int)\n",
    "                    # print(f\"SID: {SID},  depth: {depth}-1,  s2_value: {s2_value}\")\n",
    "                    node.children[i] = SID_to_util[SID]\n",
    "                else:\n",
    "                    q.append(node.children[i])\n",
    "            for i in range(len(node.children)):\n",
    "                if node.children[i] is None:\n",
    "                    print(\"missing child\")\n",
    "                    _ = node.children.pop(i)\n",
    "                    if isinstance(node, Sum):\n",
    "                        _ = node.weights.pop(i)\n",
    "                        node.weights = normalize(node.weights, norm=\"l1\")\n",
    "    work_branch = remove_unlinked(work_branch)\n",
    "    work_branch = assign_ids(work_branch)\n",
    "    max_EU = meu(work_branch, np.array([input_data]))\n",
    "    if np.all(np.isnan(input_data[1:])):\n",
    "        rspmn.branch_and_depth_to_rmeu[(branch, depth)] = max_EU\n",
    "    return max_EU\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def remove_unlinked(branch):\n",
    "    q = branch.children[1:]\n",
    "    while len(q) > 0:\n",
    "        node = q.pop(0)\n",
    "        if isinstance(node, Max) or isinstance(node, Product):\n",
    "            for i in range(len(node.children)):\n",
    "                q.append(node.children[i])\n",
    "        elif isinstance(node, Sum):\n",
    "            to_remove = []\n",
    "            for i in range(len(node.children)):\n",
    "                if terminal_sum_child(node.children[i]):\n",
    "                    print(\"removing child\")\n",
    "                    to_remove = [i]+to_remove\n",
    "                else:\n",
    "                    q.append(node.children[i])\n",
    "            for i in to_remove:\n",
    "                _ = node.children.pop(i)\n",
    "                _ = node.weights.pop(i)\n",
    "    return branch\n",
    "\n",
    "def terminal_sum_child(child):\n",
    "    q = [child]\n",
    "    while len(q) > 0:\n",
    "        node = q.pop(0)\n",
    "        if node is None: return True\n",
    "        if isinstance(node, Max) or isinstance(node, Product):\n",
    "            for i in range(len(node.children)):\n",
    "                q.append(node.children[i])\n",
    "        elif isinstance(node,Sum): return False\n",
    "\n",
    "\n",
    "\n",
    "def clear_caches(rspmn):\n",
    "    del rspmn.branch_and_depth_to_rmeu\n",
    "\n",
    "\n",
    "def get_action(branch, SID, dec_indices, num_vars=17):\n",
    "    for i in range(len(dec_indices)):\n",
    "        input_data = np.array([[np.nan]*(num_vars-len(dec_indices))+[0]*len(dec_indices)+[np.nan,SID]])\n",
    "        input_data[0][(dec_indices[i])] = 1\n",
    "        if likelihood(branch, input_data) > 0.000001:\n",
    "            return i\n",
    "    return \"noop\"\n",
    "\n",
    "\n",
    "\n",
    "def best_next_decision(rspmn, input_data, depth=1, in_place=False):\n",
    "    root = rspmn.spmn.spmn_structure\n",
    "    if in_place:\n",
    "        data = input_data\n",
    "    else:\n",
    "        data = np.copy(input_data)\n",
    "    nodes = get_nodes_by_type(root)\n",
    "    dec_dict = {}\n",
    "    # find all possible decision values\n",
    "    for node in nodes:\n",
    "        if type(node) == Max:\n",
    "            if node.dec_idx in dec_dict:\n",
    "                dec_dict[node.dec_idx].union(set(node.dec_values))\n",
    "            else:\n",
    "                dec_dict[node.dec_idx] = set(node.dec_values)\n",
    "    next_dec_idx = None\n",
    "    # find next undefined decision\n",
    "    for idx in dec_dict.keys():\n",
    "        if np.all(np.isnan(data[:,idx])):\n",
    "            next_dec_idx = idx\n",
    "            break\n",
    "    assert next_dec_idx != None, \"please assign all values of next decision to np.nan\"\n",
    "    # determine best decisions based on meu\n",
    "    dec_vals = list(dec_dict[next_dec_idx])\n",
    "    best_decisions = np.full((1,data.shape[0]),dec_vals[0])\n",
    "    data[:,next_dec_idx] = best_decisions\n",
    "    if depth == 1:\n",
    "        meu_best = meu(root, data)\n",
    "    else:\n",
    "        meu_best = np.array([rmeu(rspmn, data[0], depth)])\n",
    "    for i in range(1, len(dec_vals)):\n",
    "        decisions_i = np.full((1,data.shape[0]), dec_vals[i])\n",
    "        data[:,next_dec_idx] = decisions_i\n",
    "        if depth == 1:\n",
    "            meu_i = meu(root, data)\n",
    "        else:\n",
    "            meu_i = np.array([rmeu(rspmn, data[0], depth)])\n",
    "        best_decisions = np.select([np.greater(meu_i, meu_best),True],[decisions_i, best_decisions])\n",
    "        data[:,next_dec_idx] = best_decisions\n",
    "        meu_best = np.maximum(meu_i,meu_best)\n",
    "    return best_decisions\n",
    "\n",
    "\n",
    "\n",
    "def hard_em(rspmn, train_data):\n",
    "    train_data_unrolled = train_data.reshape((-1,train_data.shape[2]))\n",
    "    nans_em = np.empty((train_data_unrolled.shape[0],1))\n",
    "    nans_em[:] = np.nan\n",
    "    train_data_em = np.concatenate((train_data_unrolled,nans_em),axis=1)\n",
    "    print(f\"{len(rspmn.spmn.spmn_structure.children)} children\")\n",
    "    for i in range(len(rspmn.spmn.spmn_structure.children)):\n",
    "        print(f\"child {i}\")\n",
    "        branch = rspmn.spmn.spmn_structure.children[i]\n",
    "        _ = assign_ids(branch)\n",
    "        branch_SIDs = rspmn.branch_to_SIDs[branch]\n",
    "        branch_em_data = train_data_em[np.isin(train_data_em[:,0], branch_SIDs)]\n",
    "        next_SIDs = mpe(branch, branch_em_data)[:,rspmn.s2_scope_idx]\n",
    "        unique, counts = np.unique(next_SIDs, return_counts=True)\n",
    "        next_SID_counts = dict(zip(unique, counts))\n",
    "        q = branch.children[1:]\n",
    "        update_weights_hard_em(q, next_SID_counts)\n",
    "\n",
    "\n",
    "def update_weights_hard_em(q, next_SID_counts):\n",
    "    sums = []\n",
    "    count = 0\n",
    "    while(len(q) > 0):\n",
    "        node = q.pop(0)\n",
    "        if isinstance(node, Max) or isinstance(node, Product):\n",
    "            for i in range(len(node.children)):\n",
    "                q.append(node.children[i])\n",
    "        elif isinstance(node, Sum):\n",
    "            sums += [node]\n",
    "        elif isinstance(node, State):\n",
    "            count += next_SID_counts[np.argmax(node.densities)]\n",
    "    for sum_node in sums:\n",
    "        sum_counts = []\n",
    "        for i in range(len(sum_node.children)):\n",
    "            sum_counts += [update_weights_hard_em([sum_node.children[i]], next_SID_counts)]\n",
    "        sum_sum_counts = sum(sum_counts)\n",
    "        for i in range(len(sum_node.children)):\n",
    "            sum_node.weights[i] = sum_counts[i]/sum_sum_counts\n",
    "        count += sum_sum_counts\n",
    "    return count\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#################################### main ######################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load parameters\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--dataset\", default=\"crossing_traffic\")\n",
    "    parser.add_argument(\"--debug\", default=0, type=int)\n",
    "    parser.add_argument(\"--plot\", default=False, type=bool)\n",
    "    parser.add_argument(\"--apply_em\", default=False)\n",
    "    parser.add_argument(\"--mi_threshold\", default=0.01, type=float)\n",
    "    parser.add_argument(\"--deep_match\", default=True)\n",
    "    parser.add_argument(\"--horizon\", default=2, type=int)\n",
    "    parser.add_argument(\"--problem_depth\", default=10, type=int)\n",
    "    parser.add_argument(\"--samples\", default=100000, type=int)\n",
    "    parser.add_argument(\"--num_vars\", default=17, type=int)#total number of columns in dataset\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    import os, sys\n",
    "    from os import path\n",
    "    plot_path = f\"plots/{args.dataset}/{args.samples}x{args.problem_depth}/t:{args.mi_threshold}_h:{args.horizon}\"\n",
    "    if not path.exists(plot_path):\n",
    "        try:\n",
    "            os.makedirs(plot_path)\n",
    "        except OSError:\n",
    "            print (\"Creation of the directory %s failed\" % plot_path)\n",
    "            sys.exit()\n",
    "\n",
    "    num_vars = args.num_vars\n",
    "    if args.dataset == \"crossing_traffic\":\n",
    "        num_vars = 4\n",
    "    elif args.dataset == \"elevators\":\n",
    "        num_vars = 7\n",
    "    elif args.dataset == \"skill_teaching\":\n",
    "        num_vars = 10\n",
    "\n",
    "    rspmn = S_RSPMN(\n",
    "                dataset = args.dataset,\n",
    "                debug = args.debug==2,\n",
    "                debug1 = args.debug>0,\n",
    "                apply_em = args.apply_em,\n",
    "                mi_threshold = args.mi_threshold,\n",
    "                deep_match = args.deep_match,\n",
    "                horizon = args.horizon,\n",
    "                problem_depth = args.problem_depth,\n",
    "                samples = args.samples,\n",
    "                num_vars = num_vars,\n",
    "                plot_path = plot_path\n",
    "            )\n",
    "    if \"rl\" in args.dataset:\n",
    "        datapath = f\"data/{args.dataset}/{args.dataset}_{args.samples}x{args.problem_depth}x{1}.tsv\"\n",
    "    else:\n",
    "        datapath = f\"data/{args.dataset}/{args.dataset}_{args.samples}x{args.problem_depth}.tsv\"\n",
    "    df = pd.read_csv(\n",
    "        datapath,\n",
    "        index_col=0, sep='\\t',\n",
    "        header=0 if args.dataset==\"repeated_marbles\" or args.dataset==\"tiger\"  or args.dataset==\"frozen_lake\" or args.dataset==\"nchain\" or \"rl\" in args.dataset else None)\n",
    "    data = df.values.reshape(args.samples,args.problem_depth,args.num_vars)\n",
    "    data = np.around(data, decimals=2)\n",
    "\n",
    "    if args.dataset == \"crossing_traffic\":\n",
    "        decisions = data[:,:,:4]\n",
    "        decisions = np.concatenate((np.zeros((decisions.shape[0],decisions.shape[1],1)),decisions),axis=2)\n",
    "        decisions = np.argmax(decisions,axis=2)\n",
    "        robot_position = np.argmax(data[:,:,7:-1],axis=2)\n",
    "        arrival = data[:,:,5].reshape(data.shape[0],-1,1)\n",
    "        reward = data[:,:,-1]\n",
    "        data = np.concatenate(\n",
    "                (\n",
    "                    robot_position.reshape(data.shape[0],-1,1),\n",
    "                    decisions.reshape(data.shape[0],-1,1),\n",
    "                    arrival.reshape(data.shape[0],-1,1),\n",
    "                    reward.reshape(data.shape[0],-1,1),\n",
    "                ),\n",
    "                axis=2\n",
    "            )\n",
    "    elif args.dataset == \"elevators\":\n",
    "        decisions = data[:,:,:4]\n",
    "        decisions = np.concatenate((np.zeros((decisions.shape[0],decisions.shape[1],1)),decisions),axis=2)\n",
    "        decisions = np.argmax(decisions,axis=2)\n",
    "        elevator_floor = np.argmax(data[:,:,4:7],axis=2)\n",
    "        person_waiting = data[:,:,11]\n",
    "        elevator_dir = data[:,:,7]\n",
    "        person_in_elevator_down = data[:,:,8]\n",
    "        person_in_elevator_up = data[:,:,-2]\n",
    "        reward = data[:,:,-1]\n",
    "        data = np.concatenate(\n",
    "                (\n",
    "                    decisions.reshape(data.shape[0],-1,1),\n",
    "                    elevator_floor.reshape(data.shape[0],-1,1),\n",
    "                    person_in_elevator_down.reshape(data.shape[0],-1,1),\n",
    "                    elevator_dir.reshape(data.shape[0],-1,1),\n",
    "                    person_waiting.reshape(data.shape[0],-1,1),\n",
    "                    person_in_elevator_up.reshape(data.shape[0],-1,1),\n",
    "                    reward.reshape(data.shape[0],-1,1),\n",
    "                ),\n",
    "                axis=2\n",
    "            )\n",
    "    elif args.dataset == \"skill_teaching\":\n",
    "        decisions = data[:,:,:4]\n",
    "        decisions = np.concatenate((np.zeros((decisions.shape[0],decisions.shape[1],1)),decisions),axis=2)\n",
    "        decisions = np.argmax(decisions,axis=2)\n",
    "        data = np.concatenate(\n",
    "                (\n",
    "                    decisions.reshape(data.shape[0],-1,1),\n",
    "                    data[:,:,4:]\n",
    "                ),\n",
    "                axis=2\n",
    "            )\n",
    "    train_data = rspmn.learn_s_rspmn(data, plot = args.plot)\n",
    "\n",
    "    date = str(datetime.date(datetime.now()))[-5:].replace('-','')\n",
    "    hour = str(datetime.time((datetime.now())))[:2]\n",
    "\n",
    "    import pickle\n",
    "    pkle_path = f\"data/{args.dataset}/{args.samples}x{args.problem_depth}/t:{args.mi_threshold}_h:{args.horizon}\"\n",
    "    if not path.exists(pkle_path):\n",
    "        try:\n",
    "            os.makedirs(pkle_path)\n",
    "        except OSError:\n",
    "            print (\"Creation of the directory %s failed\" % pkle_path)\n",
    "            file = open(f\"data/{args.dataset}/rspmn_{date}_{hour}.pkle\",'wb')\n",
    "            pickle.dump(rspmn, file)\n",
    "            file.close()\n",
    "            data_SIDs = train_data[:,:,0].reshape(args.samples,args.problem_depth)\n",
    "            np.savetxt(f\"data/{args.dataset}/data_SIDs_{date}_{hour}.tsv\", data_SIDs, delimiter='\\t')\n",
    "    if path.exists(pkle_path):\n",
    "        file = open(f\"{pkle_path}/rspmn_{date}_{hour}.pkle\",'wb')\n",
    "        pickle.dump(rspmn, file)\n",
    "        file.close()\n",
    "        data_SIDs = train_data[:,:,0].reshape(args.samples,args.problem_depth)\n",
    "        np.savetxt(f\"{pkle_path}/data_SIDs_{date}_{hour}.tsv\", data_SIDs, delimiter='\\t')\n",
    "\n",
    "    input_data = np.array([0]+[np.nan]*(args.num_vars+1))\n",
    "    for i in range(1,args.problem_depth+1):\n",
    "        print(f\"rmeu for depth {i}:\\t\"+str(rmeu(rspmn, input_data, depth=i)))\n",
    "\n",
    "    print(f\"rmeu for depth 100:\\t\"+str(rmeu(rspmn, input_data, depth=100)))\n",
    "\n",
    "    print(\"\\napplying EM\\n\")\n",
    "    clear_caches(rspmn)\n",
    "    rspmn = hard_em(rspmn, train_data)\n",
    "    # train_data_unrolled = train_data.reshape((-1,train_data.shape[2]))\n",
    "    # nans_em = np.empty((train_data_unrolled.shape[0],1))\n",
    "    # nans_em[:] = np.nan\n",
    "    # train_data_em = np.concatenate((train_data_unrolled,nans_em),axis=1)\n",
    "    # for i in range(len(rspmn.spmn.spmn_structure.children)):\n",
    "    #     _ = assign_ids(rspmn.spmn.spmn_structure.children[i])\n",
    "    #     branch_SIDs = rspmn.branch_to_SIDs[rspmn.spmn.spmn_structure.children[i]]\n",
    "    #     branch_em_data = train_data_em[np.isin(train_data_em[:,0], branch_SIDs)]\n",
    "    #     EM_optimization(rspmn.spmn.spmn_structure.children[i], branch_em_data, skip_validation=True, iterations=1)\n",
    "\n",
    "    input_data = np.array([0]+[np.nan]*(args.num_vars+1))\n",
    "    for i in range(1,rspmn.problem_depth+1):\n",
    "        print(f\"rmeu for depth {i}:\\t\"+str(rmeu(rspmn, input_data, depth=i)))\n",
    "\n",
    "    print(f\"rmeu for depth 100:\\t\"+str(rmeu(rspmn, input_data, depth=100)))\n",
    "\n",
    "    if path.exists(pkle_path):\n",
    "        file = open(f\"{pkle_path}/rspmn_EM_{date}_{hour}.pkle\",'wb')\n",
    "        pickle.dump(rspmn, file)\n",
    "        file.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
